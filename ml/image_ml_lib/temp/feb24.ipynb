{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Function to load and preprocess an image\n",
    "def load_and_preprocess_image(image_url):\n",
    "    image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "    # Resize the image to the required dimensions for CLIP (224x224)\n",
    "    image = image.resize((224, 224))\n",
    "    return image\n",
    "\n",
    "# Function to check similarity between an image and a text\n",
    "def check_similarity(image, text, clip_model, clip_processor):\n",
    "    # Encode the image\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "    image_features = clip_model.get_image_features(**inputs)\n",
    "\n",
    "    # Encode the text\n",
    "    inputs = clip_processor(text=text, return_tensors=\"pt\")\n",
    "    text_features = clip_model.get_text_features(**inputs)\n",
    "\n",
    "    # Calculate cosine similarity between image and text features\n",
    "    similarity = torch.nn.functional.cosine_similarity(image_features, text_features)\n",
    "    return similarity.item()\n",
    "\n",
    "# Sample image URL and text\n",
    "image_url = \"http://images.cocodataset.org/val2014/COCO_val2014_000000159977.jpg\"\n",
    "text_to_compare = \"a person riding a horse\"\n",
    "\n",
    "# Load CLIP model and processor\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Load and preprocess image\n",
    "image = load_and_preprocess_image(image_url)\n",
    "\n",
    "# Check similarity between image and text\n",
    "similarity_score = check_similarity(image, text_to_compare, clip_model, clip_processor)\n",
    "print(\"Similarity score:\", similarity_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_similarity(image, \"a giraffe and zebra\", clip_model, clip_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating description: The following `model_kwargs` are not used by the model: ['features'] (note: typos in the generate arguments will also show up in this list)\n",
      "Failed to generate description.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel, GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Function to load and preprocess an image\n",
    "def load_and_preprocess_image(image_url):\n",
    "    try:\n",
    "        response = requests.get(image_url, stream=True)\n",
    "        response.raise_for_status()  # Raise error for invalid URL or unsuccessful download\n",
    "        image = Image.open(response.raw)\n",
    "        # Resize the image to the required dimensions for CLIP (224x224)\n",
    "        image = image.resize((224, 224))\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to generate text description for an image\n",
    "def generate_image_description(image, clip_model, clip_processor, gpt_model, gpt_tokenizer):\n",
    "    try:\n",
    "        # Encode the image\n",
    "        inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "        image_features = clip_model.get_image_features(**inputs)[0]  # Get the image features\n",
    "\n",
    "        # Generate text description using a separate text generation model (e.g., GPT)\n",
    "        generated_text = gpt_model.generate(\n",
    "            features=image_features.unsqueeze(0),  # Ensure the input shape is compatible with GPT-2\n",
    "            max_length=100,  # Adjust max_length as needed\n",
    "            pad_token_id=gpt_tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,  # Adjust temperature for diversity in generated text\n",
    "            top_k=50,  # Adjust top_k for diversity in generated text\n",
    "            top_p=0.95,  # Adjust top_p for diversity in generated text\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "\n",
    "        generated_text = gpt_tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating description: {e}\")\n",
    "        return None\n",
    "\n",
    "# Sample image URL\n",
    "image_url = \"http://images.cocodataset.org/val2014/COCO_val2014_000000159977.jpg\"\n",
    "\n",
    "# Load CLIP model and processor\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Load GPT model and tokenizer for text generation\n",
    "gpt_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load and preprocess image\n",
    "image = load_and_preprocess_image(image_url)\n",
    "\n",
    "if image:\n",
    "    # Generate text description for the image\n",
    "    description = generate_image_description(image, clip_model, clip_processor, gpt_model, gpt_tokenizer)\n",
    "    if description:\n",
    "        print(\"Generated Description:\", description)\n",
    "    else:\n",
    "        print(\"Failed to generate description.\")\n",
    "else:\n",
    "    print(\"Image loading failed. Check the URL and try again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=600) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features shape: torch.Size([512])\n",
      "Prompt input: tensor([  32, 4590,  286])\n",
      "Combined input shape: torch.Size([515])\n",
      "Error generating description: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)\n",
      "Failed to generate description.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel, GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Function to load and preprocess an image\n",
    "def load_and_preprocess_image(image_url):\n",
    "    try:\n",
    "        response = requests.get(image_url, stream=True)\n",
    "        response.raise_for_status()  # Raise error for invalid URL or unsuccessful download\n",
    "        image = Image.open(response.raw)\n",
    "        # Resize the image to the required dimensions for CLIP (224x224)\n",
    "        image = image.resize((224, 224))\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to generate text description for an image\n",
    "def generate_image_description(image, clip_model, clip_processor, gpt_model, gpt_tokenizer):\n",
    "    try:\n",
    "        # Encode the image\n",
    "        inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "        image_features = clip_model.get_image_features(**inputs)[0]  # Get the image features\n",
    "\n",
    "        # Encode the prompt text\n",
    "        prompt_text = \"A photo of\"\n",
    "        prompt_input = gpt_tokenizer.encode(prompt_text, return_tensors=\"pt\")[0]\n",
    "\n",
    "        # Convert prompt input to Long type\n",
    "        prompt_input = prompt_input.long()\n",
    "\n",
    "        # Concatenate prompt input with image features\n",
    "        combined_input = torch.cat([prompt_input, image_features.view(-1)])\n",
    "\n",
    "        print(\"Image features shape:\", image_features.shape)\n",
    "        print(\"Prompt input:\", prompt_input)\n",
    "        print(\"Combined input shape:\", combined_input.shape)\n",
    "\n",
    "        # Generate text description using GPT-2 model\n",
    "        generated_text = gpt_model.generate(\n",
    "            input_ids=combined_input.unsqueeze(0),\n",
    "            max_length=600,  # Adjust max_length\n",
    "            pad_token_id=gpt_tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,  # Adjust temperature for diversity in generated text\n",
    "            top_k=50,  # Adjust top_k for diversity in generated text\n",
    "            top_p=0.95,  # Adjust top_p for diversity in generated text\n",
    "            num_return_sequences=1,\n",
    "            max_new_tokens=200  # Adjust max_new_tokens to limit the number of generated tokens\n",
    "        )\n",
    "\n",
    "        generated_text = gpt_tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating description: {e}\")\n",
    "        return None\n",
    "\n",
    "# Sample image URL\n",
    "image_url = \"http://images.cocodataset.org/val2014/COCO_val2014_000000159977.jpg\"\n",
    "\n",
    "# Load CLIP model and processor\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Load GPT-2 model and tokenizer for text generation\n",
    "gpt_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load and preprocess image\n",
    "image = load_and_preprocess_image(image_url)\n",
    "\n",
    "if image:\n",
    "    # Generate text description for the image\n",
    "    description = generate_image_description(image, clip_model, clip_processor, gpt_model, gpt_tokenizer)\n",
    "    if description:\n",
    "        print(\"Generated Description:\", description)\n",
    "    else:\n",
    "        print(\"Failed to generate description.\")\n",
    "else:\n",
    "    print(\"Image loading failed. Check the URL and try again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://images.cocodataset.org/val2014/COCO_val2014_000000159977.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
